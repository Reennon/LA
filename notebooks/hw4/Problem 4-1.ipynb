{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fFDH-FC9IoFX"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "--wC2MtMI25F"
   },
   "source": [
    "# Problem 4-1: Least square solutions **(3.5 pts total)**\n",
    "\n",
    "### The aim of this lab assignment is to study various numerical methods for finding optima of objective functions. You will implement the standard methods, analyse their pros and cons, convergence rates etc.  \n",
    "\n",
    "#### Prepared by **Markiian Novosad**\n",
    "\n",
    "## Completed by:\n",
    "*   Roman Kovalchuk\n",
    "*   Eduard Pekach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mfWpXjxkKHlq"
   },
   "source": [
    "## 1. Setting of the problem and preparations **(0.25 pts)**\n",
    "\n",
    "Main features of the standard numerical methods can be illustrated on the linear regression problem of minimizing the objective function $f : \\mathbb{R}^3 \\to \\mathbb{R}$ given by  $$ f(\\beta_0, \\beta_1, \\beta_2)= \\tfrac12\\sum_{j=1}^N |\\beta_0 + \\beta_1 x_{j,1} + \\beta_2 x_{j,2} - y_j|^2\\tag{1}$$\n",
    "Here $\\beta_0$ is the intercept, $\\beta_1$ and $\\beta_2$ are the respective slopes (weights), and the datapoints $\\mathbf{x}_j = (x_{j,1},x_{j,2})$ constitute the $n\\times 2$ matrix $X$, and the vector $\\mathbf{y}$ contains $y_j$; they are written in the corresponding csv files `X.csv` and `y.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BfgsVuRvbRB4"
   },
   "outputs": [],
   "source": [
    "# Data preparation:\n",
    "# Read the csv files and write the results in the Nx2 array X and 1D array y to be used later\n",
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "    X = ...\n",
    "    y = ...\n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DxKCzNcz-hzC"
   },
   "source": [
    "These are helper functions that calculate the values of the function and its gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQKRIDcycblv"
   },
   "outputs": [],
   "source": [
    "# calculate the function\n",
    "def f(A, x, y):\n",
    "    \"\"\"\n",
    "    Calculate the value of the function f \n",
    "    Args:\n",
    "        A: 2D numpy array of shape (m, n)  \n",
    "        x: 1D numpy array of shape (n,)\n",
    "        y: 1D numpy array of shape (m,)\n",
    "    Returns:\n",
    "        nabla_f: 1D numpy array of shape (n,) representing the gradient of f\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "    assert y.ndim == 1\n",
    "    assert A.ndim == 2\n",
    "    assert A.shape[0] == y.shape[0]\n",
    "    assert A.shape[1] == x.shape[0]\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    f = ...\n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5122QMINRE7"
   },
   "outputs": [],
   "source": [
    "# function to calculate the gradient to be used throughout\n",
    "def grad(A, x, y):\n",
    "    \"\"\"\n",
    "    Calculate the gradient of the function f \n",
    "    Args:\n",
    "        A: 2D numpy array of shape (m, n)  \n",
    "        x: 1D numpy array of shape (n,)\n",
    "        y: 1D numpy array of shape (m,)\n",
    "    Returns:\n",
    "        nabla_f: 1D numpy array of shape (n,) representing the gradient of f\n",
    "    \"\"\"\n",
    "    assert x.ndim == 1\n",
    "    assert y.ndim == 1\n",
    "    assert A.ndim == 2\n",
    "    assert A.shape[0] == y.shape[0]\n",
    "    assert A.shape[1] == x.shape[0]\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    nabla_f = ...\n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return nabla_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hwi4vRsu4Zsl"
   },
   "source": [
    "## 2. Explicit solution **(0.25 pt)**\n",
    "\n",
    "The function $f$ is <font color=\"red\">convex</font>, and thus necessary and sufficient condition for the minimizer $\\mathbf{\\beta}^*$ is that the gradient $\\nabla f$ should vanish at $\\mathbf{\\beta}^*$. Before continuing the analysis, it is convenient to write $f$ as an objective function of a least square problem in matrix form\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AEm4TZqPLhRh"
   },
   "source": [
    "\n",
    "#### **Task 2.**  \n",
    "\n",
    "*   Explain why $f$ of (1) can be written as $$ f(\\beta) = \\tfrac12\\|A \\beta - \\mathbf{y}\\|^2 $$\n",
    "and identify the matrix $A$. Do you see how $X$ and $A$ are related? \n",
    "---\n",
    "**Your explanations here**\n",
    "\n",
    "---\n",
    "*   Write the gradient in explicit form and solve the equation $\\nabla f(\\mathbf{x}) = \\mathbf{0}$. What assumption on $A$ guarantees that the solution exists and is unique? \n",
    "---\n",
    "**Your explanations here**\n",
    "\n",
    "---\n",
    "*   Implement the solution $\\beta$ by solving the equation $\\nabla f(\\mathbf{x}) = \\mathbf{0}$ and compare the result with the solution $\\beta_0$ using the built-in function in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FU0JToXZQdjb"
   },
   "outputs": [],
   "source": [
    "# Calculate the solution beta \n",
    "def solve_normal(A, y):\n",
    "    \"\"\"\n",
    "    Calculate the weights in the linear regression via normal equation \n",
    "    Args:\n",
    "        A: 2D numpy array of shape (m, n)  \n",
    "        y: 1D numpy array of shape (m,)\n",
    "    Returns:\n",
    "        beta: 1D numpy array of shape (n,) representing the gradient of f\n",
    "    \"\"\"\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    beta = ...\n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qTVpQaiNc-9a"
   },
   "outputs": [],
   "source": [
    "# compare the solutions\n",
    "A = ...\n",
    "beta_0 = np.linalg.lstsq(A, y)\n",
    "solve_normal(A, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ICejHJyFY2CW"
   },
   "outputs": [],
   "source": [
    "def check_solutions(A, y):\n",
    "    beta = solve_normal(A, y)    \n",
    "    assert np.isclose(beta, beta_0).all(), f'beta != beta_0;\\n(beta - beta_0)=\\n{beta - beta_0}'\n",
    "    print('The solution beta is correct')\n",
    "    \n",
    "check_solutions(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YPNbduR3hu9j"
   },
   "source": [
    "## 3. Gradient descent with constant stepsize **(1 pt)**\n",
    "\n",
    "The descent methods iteratively update approximation $\\mathbf{\\beta}_k$ in a <font color=\"red\">search direction</font> $\\mathbf{d}$ and with a <font color=\"red\">stepsize</font> $t$ via $\\mathbf{\\beta}_{k+1} := \\mathbf{\\beta}_k + t \\mathbf{d}$. In the <font color=\"red\">constant</font> step size approach, the stepsize is fixed in advance. \n",
    "\n",
    "An obvious drawback of such an approach is that if that initial stepsize is not small enough, the approach may not converge as the new value of $f$ can be larger than the current one. A natural modification goes as follows: each iteration starts with a fixed value of $t$, which is halved consecutively until the next approximation of $\\beta$ decreases the current value of $f$. \n",
    "\n",
    "In this task, you will analyze how the choice of the stepsize affects the number of iterations until convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZdcjQCvUjkbX"
   },
   "source": [
    "#### **Task 3.1** \n",
    "\n",
    "\n",
    "*   Implement the functions calculating the gradient and performing the gradient descent with constant stepsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PMhEGYStj-x5"
   },
   "outputs": [],
   "source": [
    "# gradient descent with constant stepsize\n",
    "def const_step(A, y, stepsize, tol = 1e-6, limit=1e4):\n",
    "    \"\"\"\n",
    "    Implement the gradient descent with constant stepsize\n",
    "    Args:\n",
    "        A: 2D numpy array of shape (m, n)  \n",
    "        y: 1D numpy array of shape (m,)\n",
    "        stepsize: constant stepsize\n",
    "        tol: relative precision of change in x in the stopping criterion\n",
    "        limit: maximum number of iteration (adjust as necessary)\n",
    "    Returns:\n",
    "        x: 1D numpy array of shape (n,) giving the least square solution of Ax=y\n",
    "    \"\"\"\n",
    "    x = np.zeros(len(A[0, :]))  # initialize x\n",
    "    n = 0                       # initialize number of iterations\n",
    "    rel_update = 2*tol          # initialize the relative update \n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    while  # stopping criteria are not met\n",
    "    ...    \n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return (x, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d76ds5wc_I7Z"
   },
   "outputs": [],
   "source": [
    "# compare the solutions\n",
    "def check_solutions_const_step(A, y, stepsize):\n",
    "    beta = const_step(A, y, stepsize)[0]\n",
    "    assert np.isclose(beta, beta_0).all(), f'beta != beta_0;\\n(beta - beta_0)=\\n{beta - beta_0}'\n",
    "    print('The solution beta is correct')\n",
    "\n",
    "stepsize = ...                  # use the stepsize that guarantee convergence\n",
    "const_step(A, y, stepsize)\n",
    "\n",
    "check_solutions_const_step(A, y, stepsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "No7_oOMjfnVB"
   },
   "source": [
    "\n",
    "\n",
    "*   Analyze the dependence of the stepsize and the number of iterations needed until convergence; plot the graph. Think of reasonable interval of stepsizes. Comment on the optimal stepsize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2A7xZrEogQ7X"
   },
   "outputs": [],
   "source": [
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "...   \n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V610nRKZgzJe"
   },
   "source": [
    "### Task 3.2.\n",
    "\n",
    "*   The above approach has a serious drawback: very often, the updated $\\mathbf{x}^+$ increases the value of $f$, i.e., $f(\\mathbf{x}^+) > f(\\mathbf{x})$. The following ''backtracking'' modification seems natural: on each iteration, we start with the fixed value of $t$, and then halve it until $f(\\mathbf{x} + t \\mathbf{d}) < f(\\mathbf{x})$. Implement this modification\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1El0C8yE3nJZ"
   },
   "outputs": [],
   "source": [
    "# gradient descent with variable stepsize\n",
    "def const_step_bctr(A, y, stepsize, tol = 1e-6, limit=1e4):\n",
    "    \"\"\"\n",
    "    Implement the gradient descent with constant stepsize\n",
    "    Args:\n",
    "        A: 2D numpy array of shape (m, n)  \n",
    "        y: 1D numpy array of shape (m,)\n",
    "        stepsize: constant stepsize\n",
    "        tol: relative precision of change in x in the stopping criterion\n",
    "        limit: maximum number of iteration (adjust as necessary)\n",
    "    Returns:\n",
    "        x: 1D numpy array of shape (n,) giving the least square solution of Ax=y\n",
    "    \"\"\"\n",
    "    x = np.zeros(len(A[0, :]))  # initialize x\n",
    "    n = 0                       # initialize number of iterations\n",
    "    rel_update = 2*tol          # initialize the relative update \n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    while  # stopping criteria are not met\n",
    "        t = stepsize            # initialize the stepsize\n",
    "        x_new = ...\n",
    "        while ...\n",
    "            t = t/2\n",
    "        ...    \n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return (x, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zDdpfqKwbK9x"
   },
   "outputs": [],
   "source": [
    "def check_solutions_const_step_bctr(A, y):\n",
    "    beta = const_step_bctr(A, y, stepsize)[0]\n",
    "    assert np.isclose(beta, beta_0).all(), f'beta != beta_0;\\n(beta - beta_0)=\\n{beta - beta_0}'\n",
    "    print('The solution beta is correct')\n",
    "\n",
    "stepsize = ...                  # use the stepsize that guarantee convergence\n",
    "const_step_bctr(A, b, stepsize)\n",
    "\n",
    "check_solutions_const_step_bctr(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0KbvCGL0xeOC"
   },
   "source": [
    "### Task 3.3\n",
    "\n",
    "- discuss the effect on convergence and optimal value of the stepsize.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BzhfuCX3rW0"
   },
   "source": [
    "---\n",
    "#### **Your explanations here**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eowArJ8S6IPC"
   },
   "source": [
    "## 4. Exact line search **(1 pt)**\n",
    "\n",
    "In the exact line search approach, the stepsize $t$ is chosen optimally on each iteration, in the sense that $t=t^+$ should minimize the function $g(t):=f(\\mathbf{x} + t \\mathbf{d})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpHr9KWp7KEJ"
   },
   "source": [
    "### Task 4.1 \n",
    "\n",
    "\n",
    "*   Derive the explicit formula for the stepsize $t^+$, the iteration $\\mathbf{x}^+ = \\mathbf{x} - t^+ \\nabla f(\\mathbf{x})$, and the updated value $f(\\mathbf{x}^+)$ for the linear regression model of (1). Derive the bound on the constant $c$ such that $f(\\mathbf{x}^+) \\le c f(\\mathbf{x})$ in terms of the spectral bounds on $A^\\top A$. \n",
    "---\n",
    "#### **Your explanations here**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "umMejfcOIiQV"
   },
   "source": [
    "*   Implement the gradient descent method with exact line search for the linear regression problem of minimizing (1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rzZFztJm86RT"
   },
   "outputs": [],
   "source": [
    "# gradient descent with exact line search\n",
    "def exact_search(A, y, tol = 1e-6, limit=1e4):\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    ...   \n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return (x, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZaEmMllu-Vpu"
   },
   "outputs": [],
   "source": [
    "# check that the solution coincides with beta_0 calculated through the built-in function\n",
    "def check_solutions_exact_search(A, y):\n",
    "    beta = exact_search(A, y)[0]\n",
    "    assert np.isclose(beta, beta_0).all(), f'beta != beta_0;\\n(beta - beta_0)=\\n{beta - beta_0}'\n",
    "    print('The solution beta is correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D4x5j3XtcqqJ"
   },
   "outputs": [],
   "source": [
    "exact_search(A, y)\n",
    "check_solutions_exact_search(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qPj1BEzIbI0"
   },
   "source": [
    "### Task 4.2\n",
    "\n",
    "*   Demonstrate the zig-zag behaviour of the iterations: check numerically that the two consecutive directions, $\\nabla f(\\mathbf{x})$ and $\\nabla f(\\mathbf{x}^+)$ are orthogonal in $\\mathbb{R}^3$ on two different iterations\n",
    "---\n",
    "#### **Your explanations here (can include the code)**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yc48m0HdI3XI"
   },
   "source": [
    "### Task 4.3 \n",
    "\n",
    "*   Look at the number of iterations made and compare it with that using the constant stepsize (and its ''backtraking'' modification)\n",
    "---\n",
    "#### **Your arguments here**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7jpH-m3JEyV"
   },
   "source": [
    "*   Analyze the convergence rate: discuss the reduction factors $f(\\mathbf{x}_{k})- f(\\mathbf{x}_{k+1})/(f(\\mathbf{x}_{k} - f(\\mathbf{x}_{k-1}))$ and check if it tends to the predicted value $c = (M-m)^2/(M+m)^2$, with $M$ and $m$ the largest and the smallest eigenvalues of the matrix $A^\\top A$.\n",
    "---\n",
    "#### **Your arguments here**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-Vw18t2091ur"
   },
   "source": [
    "### Task 4.4\n",
    "\n",
    "*   Implement the accelerated method that does not suffer from the \"zig-zag\" behaviour via $$\\mathbf{x}^+ = \\mathbf{x} - t \\mathbf{d^+}, \\qquad \\mathbf{d}^+ = \\nabla f(\\mathbf{x}) - s \\mathbf{d}$$ with a fixed damping factor $s$ and $t$ obtained by exact line search method. Analyze convergence speed (number of iterations required) as compared to the classic exact line search method.  \n",
    "---\n",
    "#### **Your arguments here**\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQSs-XnyJmwc"
   },
   "outputs": [],
   "source": [
    "#  accelerated exact line search method\n",
    "def exact_search_accel(A, y, tol = 1e-6, limit=1e4):\n",
    "    # ========= YOUR CODE STARTS HERE ========= #\n",
    "    ...   \n",
    "    # ========== YOUR CODE ENDS HERE ========== #\n",
    "    return (x, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3ePTQdtc5Bt"
   },
   "outputs": [],
   "source": [
    "# check that the solution coincides with beta_0 calculated through the built-in function\n",
    "def check_solutions_exact_search_accel(A, y):\n",
    "    beta = exact_search_accel(A, y, stepsize)[0]\n",
    "    assert np.isclose(beta, beta_0).all(), f'beta != beta_0;\\n(beta - beta_0)=\\n{beta - beta_0}'\n",
    "    print('The solution beta is correct')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H6MHk1WedFHY"
   },
   "outputs": [],
   "source": [
    "exact_search_accel(A, y)\n",
    "check_solutions_exact_search_accel(A, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hOV8iQDMJQ8v"
   },
   "source": [
    "*   Analyze the convergence rate: discuss the reduction factors $(f(\\mathbf{x}_{k})- f(\\mathbf{x}_{k+1}))/(f(\\mathbf{x}_{k}) - f(\\mathbf{x}_{k-1}))$ and check if it tends to the predicted value $c = (M-m)^2/(M+m)^2$, with $M$ and $m$ the largest and the smallest eigenvalues of the matrix $A^\\top A$.\n",
    "---\n",
    "#### **Your arguments here**\n",
    "\n",
    "--- \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-o9Cbc9jNc-3"
   },
   "source": [
    "## 5. Backtracking line search **(0.25 pt)**\n",
    "\n",
    "This is a compromise between the constant stepsize and exact line search approaches. We start with a fixed value of $t=1$, and unless $f(\\mathbf{x}^+)$ has not decreased \"enough\", we scale $t$ to $\\beta t$, with some $\\beta <1 $ that is not too small. The decision to accept $t$ is based on the condition $f(\\mathbf{x}^+) < f(\\mathbf{x}) - \\alpha t \\|\\nabla f(\\mathbf{x})\\|^2$ with $\\alpha \\in (0,\\tfrac12)$; this guarantees that the stepsize does not become too small while decreasing the value of $f$ linearly in $t$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JW1TXV9bP93M"
   },
   "source": [
    "### Task 5.1\n",
    "\n",
    "*   Implement the backtracing line search method and check the solution vs $\\beta_0$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6OR6w4YeQJNJ"
   },
   "outputs": [],
   "source": [
    "# backtracking line search\n",
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "    ...   \n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KDrPIa1kQWth"
   },
   "source": [
    "\n",
    "*   Analyse eficiency of the method depending on the parameters $\\alpha$ and $\\beta$. Comment on the optimal values of the parameters. Compare the backtracking with the exact line search method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QHnEdGwtK0NL"
   },
   "source": [
    "## 6. Stochastic gradient descent **(0.5 pt)**\n",
    "\n",
    "Very often, the objective function $f$ is the sum $\\sum_j f_j$, so that the gradient $\\nabla f$ is the sum of $\\nabla f_j$. If the computation of each gradient $\\nabla f_j$ is costly, then it makes sense not to compute the whole sum but choose randomly several summands (their number $k$ is called the <font color=\"red\">batch size</font>), hoping that on average the directions will be correct. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gE-w0IevRDWs"
   },
   "source": [
    "### Task 6.1 \n",
    "*   Implement the stocahstic gradient descent method with variable batch size $k=1, 5, 10$. Take a sufficiently small constant stepsize (base your choice on Task 1.2). Analyze dependence of the iteration numbers on the batch size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-aHfXJair0j"
   },
   "outputs": [],
   "source": [
    "# stochastic gradient descent\n",
    "# ========= YOUR CODE STARTS HERE ========= #\n",
    "    ...   \n",
    "# ========== YOUR CODE ENDS HERE ========== #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m7cnrJg_izlU"
   },
   "source": [
    "## 7. Conclusions **(0.25 pt)**\n",
    "\n",
    "Summarize in a few sentences what you have learned and achieved by completing the tasks of this assignment. Comment on how this assignment can be imporved in the future\n",
    "\n",
    "---\n",
    "#### **Your arguments here**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
