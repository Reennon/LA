{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "fFDH-FC9IoFX",
    "ExecuteTime": {
     "end_time": "2024-02-16T16:49:34.269884Z",
     "start_time": "2024-02-16T16:49:34.268280Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Lab Assignment 4.2: Distance to the ellipsoid **(2.5 pts total)**\n",
    "\n",
    "### The aim of this lab assignment is to apply several numerical methods for finding the distance to the ellipsoid and evaluate their effectiveness. You will implement the standard methods (descent, Newton a.o.), analyse their pros and cons, convergence rates etc.  \n",
    "\n",
    "#### Prepared by: **Volodymyr Kuchynskyy**\n",
    "## Completed by:\n",
    "*   Roman Kovalchuk\n",
    "*   Eduard Pekach"
   ],
   "metadata": {
    "id": "--wC2MtMI25F"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##2.1. Setting and preliminaries (0.25 pts)\n",
    "\n",
    "- The ellipsoid surface $E$ is given by the equation $$\\frac{x^2}{a^2} + \\frac{y^2}{b^2} + \\frac{z^2}{c^2} = 1,$$ where $a$, $b$, and $c$ are semi-axes in the respective directions.\n",
    "- the task is to find the point on $E$ that is the closest one to a given point $\\mathbf{y} = (x_0, y_0, z_0)$ outside the ellipsoid\n",
    "\n",
    "\n",
    "###Task 2.1\n",
    "Write down the objective function $f(\\mathbf{x})$ and the constraint function $g(\\mathbf{x})$ (so that $E$ coincides with $\\{\\mathbf{x}\\in \\mathbb{R}^3 \\mid g(\\mathbf{x})=0\\}$). Calculate the gradients explicitly\n"
   ],
   "metadata": {
    "id": "PiNJvnpNj5Xy"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "**Write here the objective, and constraint functions and calculations of the gradient**\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "Ig-27SmWx971"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#declare and define your *objective* and *constaint* functions here if necessary\n",
    "#the functions *must* accept the point x Ñ” R^3 (np.array or Tuple) as the first\n",
    "#argument\n",
    "#...\n",
    "f_x = lambda x, x_0: (x[0] - x_0[0])**2 + (x[1] - x_0[1])**2 + (x[2] - x_0[2])**2\n",
    "g_x = lambda x, a, b, c: x[0]**2/a**2 + x[1]**2/b**2 + x[2]**2/c**2 - 1# your constraint function (either lambda or defined above)"
   ],
   "metadata": {
    "id": "EzxzEMTVGExy",
    "ExecuteTime": {
     "end_time": "2024-02-16T17:21:46.745693Z",
     "start_time": "2024-02-16T17:21:46.728235Z"
    }
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 Gradient descent methods (1 pts)"
   ],
   "metadata": {
    "id": "44U1WCbDyTE_"
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "id": "ajnW2gc1ofUW"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2.2: gradient descent method\n",
    "Implement the gradient descent method to find the point $\\mathbf{x}_0 \\in E$ that is the closest one to $\\mathbf{y}$.\n",
    "\n",
    "\n",
    "*   Start with the point $\\mathbf{x}_0$ that lies on the intersection of $O\\mathbf{y}$ and $E$;\n",
    "*   observe that the negative gradient of $f$ points towards $\\mathbf{y}$ and thus outside the ellipsoid surface. Show that the best descent direction $\\mathbf{d}$ is the projection of $-\\nabla f$ onto the tangent plane to $E$ at $\\mathbf{x}$\n",
    "*   implement one iteration using the constant stepsize $t^+$ that is halved as necessary until $f(\\mathbf{x}^+)< f(\\mathbf{x})$. Do not forget to project the point $\\mathbf{x} + t^+ \\mathbf{d}$ onto the ellipsoid surface to get $\\mathbf{x}^+$\n",
    "*   test your solution on 100 random realizations of the semi-axes $a, b$, and $c$ uniformly distributed in $[1, 5]$ and a random point $\\mathbf{y}$ with entries $(x,y,z)$ uniformly distributed in $[5,10]$; discuss the convergence rate (number of iterations needed and decrease in $f(\\mathbf{x}_k)$\n",
    "\n"
   ],
   "metadata": {
    "id": "ViU37OamcLdn"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of iterations needed: 100.0\n",
      "Average decrease in f(x_k): 11.481961887004708\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def constrained_grad_descent(f_x, g_x, E_params, y, iterations=None):\n",
    "    \"\"\"Perform gradient descent for constrained optimization problem.\n",
    "    Args:\n",
    "        f_x: the function for which the gradient is taken\n",
    "        g_x: the constraint function\n",
    "        E_params (Tuple(3)): the semi-axes (parameters) of the ellipsoid E. Used to\n",
    "                            find direction *d*\n",
    "        y (Tuple(3)): the point for which the closest point on an ellipsoid has to be found\n",
    "        iterations (int or None): the maximum number of method iterations,\n",
    "                                  after which the algorithm will stop even when the necessary\n",
    "                                  conditions aren't met. None = unlimited\n",
    "\n",
    "    Returns:\n",
    "        Tuple(x_0, iter_count): return a pair, containing the point x_0 which is\n",
    "                                the closest to y and the number of\n",
    "                                iterations taken for this\n",
    "    \"\"\"\n",
    "    x_0 = np.random.uniform(5, 10, (num_samples, 3))[0]\n",
    "    iter_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Calculate gradient of f_x at x_0\n",
    "        grad_f = np.array([2 * (x_0[i] - y[i]) for i in range(len(x_0))])\n",
    "\n",
    "        # Calculate the projection of the negative gradient onto the tangent plane\n",
    "        grad_f_norm = grad_f / np.linalg.norm(grad_f)\n",
    "        grad_f_squared = np.dot(grad_f, grad_f)\n",
    "        direction = grad_f_norm * np.sqrt(grad_f_squared)\n",
    "\n",
    "        # Perform one iteration of gradient descent\n",
    "        alpha = 1.0  # Constant step size\n",
    "        x_plus = x_0 - alpha * direction\n",
    "\n",
    "        # Project x_plus onto the ellipsoid surface\n",
    "        a, b, c = E_params\n",
    "        x_plus_normalized = np.array([x_plus[i] / E_params[i] for i in range(len(x_plus))])\n",
    "        norm_x_plus = np.linalg.norm(x_plus_normalized)\n",
    "        x_plus = np.array([a * x_plus[0] / norm_x_plus, b * x_plus[1] / norm_x_plus, c * x_plus[2] / norm_x_plus])\n",
    "\n",
    "        # Check convergence\n",
    "        if f_x(x_plus, y) < f_x(x_0, y):\n",
    "            x_0 = x_plus\n",
    "\n",
    "        iter_count += 1\n",
    "        if iterations is not None and iter_count >= iterations:\n",
    "            break\n",
    "\n",
    "    return x_0, iter_count\n",
    "\n",
    "# Test the method with proper number of iterations and semi-axes/points sampled\n",
    "num_iterations = 100\n",
    "num_samples = 100\n",
    "a_values = np.random.uniform(1, 5, num_samples)\n",
    "b_values = np.random.uniform(1, 5, num_samples)\n",
    "c_values = np.random.uniform(1, 5, num_samples)\n",
    "y_values = np.random.uniform(5, 10, (num_samples, 3))\n",
    "\n",
    "total_iters = 0\n",
    "total_decrease = 0.0\n",
    "for i in range(num_samples):\n",
    "    E_params = (a_values[i], b_values[i], c_values[i])\n",
    "    y = y_values[i]\n",
    "    x_0, iter_count = constrained_grad_descent(f_x, g_x, E_params, y, iterations=num_iterations)\n",
    "    total_iters += iter_count\n",
    "    total_decrease += f_x(x_0, y)\n",
    "\n",
    "avg_iters = total_iters / num_samples\n",
    "avg_decrease = total_decrease / num_samples\n",
    "print(\"Average number of iterations needed:\", avg_iters)\n",
    "print(\"Average decrease in f(x_k):\", avg_decrease)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T22:43:11.615373Z",
     "start_time": "2024-02-16T22:43:11.452767Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distance is: -2.1882948811359935\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([5.15397755, 6.62167082, 6.43596101])"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Distance is:\", np.linalg.norm(x_0) - np.linalg.norm(y))\n",
    "x_0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T22:43:13.227943Z",
     "start_time": "2024-02-16T22:43:13.221935Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "\n",
    "\n",
    "## 2.3 The problem in spherical coordinates (1 pt)"
   ],
   "metadata": {
    "id": "KcmSw7q6KvdU"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2.3: reduction to unconstrained minimization problem (0.5 pts)\n",
    "Alternatively, the ellipsoid $E$ can be parameterised by two angles $\\phi$ and $\\psi$ in the following way:  $$x = a \\cos\\phi \\sin \\psi, \\qquad y = b \\sin\\phi \\sin\\psi, \\qquad z = c \\cos\\psi $$with $\\phi \\in [0,2\\pi]$ and $\\psi \\in [0,\\pi]$.\n",
    "- Formulate the above problem as an unconstrained minimization problem for some objective function $f$ of arguments $\\phi$ and $\\psi$.\n",
    "- Write down $f$ explicitly, then write the first order necessary condition for minimizer.\n",
    "- Set up the gradient descent method to find the minimizer using the backtracking line search approach\n",
    "- Test your solution on 100 random realizations with random $\\phi, \\psi\\in (0,\\pi/2)$ and a fixed point $\\mathbf{y}$ outside the ellipsoid $E$\n",
    "- Discuss the convergence rate and number of steps needed\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "nilSUmLljVXH"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def f_x_polar(phi, psi, y, a, b, c):\n",
    "    \"\"\"Objective function f parameterized by phi and psi.\"\"\"\n",
    "    term1 = a * np.cos(phi) * np.sin(psi) - y[0]\n",
    "    term2 = b * np.sin(phi) * np.sin(psi) - y[1]\n",
    "    term3 = c * np.cos(psi) - y[2]\n",
    "    return term1**2 - term2**2 - term3**2\n"
   ],
   "metadata": {
    "id": "cS-ADkXDIx2U",
    "ExecuteTime": {
     "end_time": "2024-02-16T22:43:57.211147Z",
     "start_time": "2024-02-16T22:43:57.207356Z"
    }
   },
   "execution_count": 44,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def gradient_f_x_polar(phi, psi, y, a, b, c):\n",
    "    \"\"\"Gradient of objective function f with respect to phi and psi.\"\"\"\n",
    "    term1 = a * np.cos(phi) * np.sin(psi) - y[0]\n",
    "    term2 = b * np.sin(phi) * np.sin(psi) - y[1]\n",
    "    term3 = c * np.cos(psi) - y[2]\n",
    "\n",
    "    df_dphi = -2 * a * term1 * np.sin(phi) * np.sin(psi) - 2 * b * term2 * np.cos(phi) * np.sin(psi)\n",
    "    df_dpsi = 2 * a * term1 * np.cos(phi) * np.cos(psi) - 2 * b * term2 * np.sin(phi) * np.cos(psi) - 2 * c * term3 * np.sin(psi)\n",
    "\n",
    "    return df_dphi, df_dpsi\n",
    "\n",
    "def line_search_grad_descent(f_x, grad_f_x, polar_params, E_params, y, iterations=10):\n",
    "    \"\"\"Perform gradient descent for unconstrained optimization problem using backtracking line search.\n",
    "    Args:\n",
    "        f_x: the function for which the gradient is taken\n",
    "        grad_f_x: the gradient function\n",
    "        polar_params(Tuple): in polar coodinates, used to define x,y and z.\n",
    "        E_params(Tuple): the semi-axes (parameters) of the ellipsoid E. Used to\n",
    "                            find the values of x, y and z\n",
    "        y(Tuple): the point for which the closest point on an ellipsoid has to be found\n",
    "        iterations(int or None): the maximum number of method iterations,\n",
    "                    after which the algorithm will stop even when the necessary\n",
    "                    conditions arent met. None = unlimited\n",
    "\n",
    "    Returns:\n",
    "        Tuple(x_0, iter_count): return a pair, containing the point x_0 which is\n",
    "                                the closest to y and the number of\n",
    "                                iterations taken for this\n",
    "    \"\"\"\n",
    "    phi, psi = polar_params\n",
    "    a, b, c = E_params\n",
    "    iter_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Compute gradient of f with respect to phi and psi\n",
    "        grad_phi, grad_psi = grad_f_x(phi, psi, y, a, b, c)\n",
    "\n",
    "        # Perform backtracking line search to find optimal step size\n",
    "        alpha = 1.0\n",
    "        while f_x(phi - alpha * grad_phi, psi - alpha * grad_psi, y, a, b, c) > f_x(phi, psi, y, a, b, c) - 0.5 * alpha * (grad_phi**2 + grad_psi**2):\n",
    "            alpha *= 0.5\n",
    "\n",
    "        # Update phi and psi\n",
    "        phi -= alpha * grad_phi\n",
    "        psi -= alpha * grad_psi\n",
    "\n",
    "        iter_count += 1\n",
    "        if iterations is not None and iter_count >= iterations:\n",
    "            break\n",
    "\n",
    "    return (phi, psi), iter_count\n",
    "\n",
    "# Test the method with 100 random realizations\n",
    "num_samples = 100\n",
    "y_values = np.random.uniform(5, 10, (num_samples, 3))\n",
    "a, b, c = np.random.uniform(1, 5, (3,))\n",
    "total_iters = 0\n",
    "\n",
    "for i in range(num_samples):\n",
    "    y = y_values[i]\n",
    "    phi_initial, psi_initial = np.random.uniform(0, np.pi/2, (2,))\n",
    "    polar_params = (phi_initial, psi_initial)\n",
    "    x_0, iter_count = line_search_grad_descent(f_x_polar, gradient_f_x_polar, polar_params, (a, b, c), y, 100)\n",
    "    total_iters += iter_count\n",
    "\n",
    "avg_iters = total_iters / num_samples\n",
    "print(\"Average number of iterations needed:\", avg_iters)"
   ],
   "metadata": {
    "id": "jmJH6zMVkcbm",
    "ExecuteTime": {
     "end_time": "2024-02-16T22:49:32.400717Z",
     "start_time": "2024-02-16T22:49:28.492976Z"
    }
   },
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of iterations needed: 100.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.12407794416301623, 2.9026867894595894)"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T22:49:32.405094Z",
     "start_time": "2024-02-16T22:49:32.401358Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "In this modified implementation, the objective function and its gradient are computed directly in terms of the polar coordinates \\( \\phi \\) and \\( \\psi \\). This is particularly useful when dealing with geometrical problems where the coordinates are naturally represented in polar form, as it simplifies the computation and maintains consistency with the problem domain.\n",
    "\n",
    "In contrast, when Cartesian coordinates are used, the objective function and its gradient are typically expressed in terms of \\( x \\), \\( y \\), and \\( z \\). While Cartesian coordinates offer simplicity in certain calculations, they may not always align with the inherent geometry of the problem, especially when dealing with ellipsoids or other curved surfaces.\n",
    "\n",
    "By utilizing polar coordinates, we ensure that \\( \\phi \\) and \\( \\psi \\) can vary independently, allowing for a more flexible representation of the problem space. This flexibility can be advantageous in scenarios where the ellipsoid's orientation or shape varies, leading to more robust optimization results.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "e_OR-ADjIZpE"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Task 2.4: Newton method (0.5 pt)\n",
    "\n",
    "The Newton method is the second order optimization method, which tries to find the critical point, i.e., solution to $\\nabla f(\\theta) = 0$, with $\\theta = (\\phi,\\psi)$. The idea is to use the gradient descent approach to solve the latter equation: $$\\nabla f(\\theta + \\Delta \\theta) \\approx \\textbf{0} \\iff \\nabla f(\\theta) + \\nabla^2 f(\\theta) \\, \\Delta \\theta \\approx \\textbf{0} \\iff \\Delta \\theta \\approx - \\bigl(\\nabla^2 f(\\theta)\\bigr)^{-1} \\nabla f(\\theta)$$\n",
    "The methods works well when the objective function $f$ is concave\n",
    "\n",
    "- Implement the Newton method\n",
    "- Test your implementation on 100 initial $\\theta = (\\phi,\\psi)$ with random $\\phi, \\psi \\in (0,\\pi/2)$ and a fixed point $\\mathbf{y}$ outside the ellipsoid $E$\n",
    "- Discuss the convergence rate and number of steps needed\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "Hj5ACGs5khHS"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "def f_x_polar(phi, psi, y, a, b, c):\n",
    "    \"\"\"Objective function f parameterized by phi and psi.\"\"\"\n",
    "    term1 = a * np.cos(phi) * np.sin(psi) - y[0]\n",
    "    term2 = b * np.sin(phi) * np.sin(psi) - y[1]\n",
    "    term3 = c * np.cos(psi) - y[2]\n",
    "    return term1**2 - term2**2 - term3**2\n",
    "\n",
    "def gradient_f_x_polar(phi, psi, y, a, b, c):\n",
    "    \"\"\"Gradient of objective function f with respect to phi and psi.\"\"\"\n",
    "    term1 = a * np.cos(phi) * np.sin(psi) - y[0]\n",
    "    term2 = b * np.sin(phi) * np.sin(psi) - y[1]\n",
    "    term3 = c * np.cos(psi) - y[2]\n",
    "\n",
    "    df_dphi = -2 * a * term1 * np.sin(phi) * np.sin(psi) - 2 * b * term2 * np.cos(phi) * np.sin(psi)\n",
    "    df_dpsi = 2 * a * term1 * np.cos(phi) * np.cos(psi) - 2 * b * term2 * np.sin(phi) * np.cos(psi) - 2 * c * term3 * np.sin(psi)\n",
    "\n",
    "    return df_dphi, df_dpsi\n",
    "\n",
    "def hessian_f_x_polar(phi, psi, y, a, b, c):\n",
    "    \"\"\"Hessian matrix of objective function f with respect to phi and psi.\"\"\"\n",
    "    term1 = a * np.cos(phi) * np.sin(psi) - y[0]\n",
    "    term2 = b * np.sin(phi) * np.sin(psi) - y[1]\n",
    "    term3 = c * np.cos(psi) - y[2]\n",
    "\n",
    "    d2f_dphi2 = -2 * a * term1 * np.cos(phi) * np.sin(psi) + 2 * b * term2 * np.sin(phi) * np.sin(psi)\n",
    "    d2f_dphidpsi = -2 * a * term1 * np.sin(phi) * np.cos(psi) - 2 * b * term2 * np.cos(phi) * np.cos(psi)\n",
    "    d2f_dpsidphi = d2f_dphidpsi\n",
    "    d2f_dpsi2 = -2 * a * term1 * np.cos(phi) * np.sin(psi) + 2 * b * term2 * np.sin(phi) * np.sin(psi) + 2 * c * term3 * np.cos(psi)\n",
    "\n",
    "    return np.array([[d2f_dphi2, d2f_dphidpsi], [d2f_dpsidphi, d2f_dpsi2]])\n",
    "\n",
    "def newton_method_grad_descent(f_x, gradient_f_x, hessian_f_x, polar_params, E_params, y, iterations=None):\n",
    "    \"\"\"Perform Newton's method for unconstrained optimization problem.\n",
    "    Args:\n",
    "        f_x: the objective function\n",
    "        gradient_f_x: the gradient of the objective function\n",
    "        hessian_f_x: the Hessian matrix of the objective function\n",
    "        polar_params(Tuple): in polar coordinates, used to define x, y, and z.\n",
    "        E_params(Tuple): the semi-axes (parameters) of the ellipsoid E. Used to\n",
    "                            find the values of x, y, and z\n",
    "        y(Tuple): the point for which the closest point on an ellipsoid has to be found\n",
    "        iterations(int or None): the maximum number of method iterations,\n",
    "                    after which the algorithm will stop even when the necessary\n",
    "                    conditions aren't met. None = unlimited\n",
    "\n",
    "    Returns:\n",
    "        Tuple(x_0, iter_count): a pair containing the point x_0 which is\n",
    "                                the closest to y and the number of\n",
    "                                iterations taken for this\n",
    "    \"\"\"\n",
    "    phi, psi = polar_params\n",
    "    a, b, c = E_params\n",
    "    iter_count = 0\n",
    "\n",
    "    while True:\n",
    "        # Compute gradient and Hessian of f at current point\n",
    "        grad_f = np.array(gradient_f_x(phi, psi, y, a, b, c))\n",
    "        hessian_f = hessian_f_x(phi, psi, y, a, b, c)\n",
    "\n",
    "        # Compute Newton step\n",
    "        newton_step = -np.linalg.inv(hessian_f).dot(grad_f)\n",
    "\n",
    "        # Update parameters\n",
    "        phi += newton_step[0]\n",
    "        psi += newton_step[1]\n",
    "\n",
    "        iter_count += 1\n",
    "        if iterations is not None and iter_count >= iterations:\n",
    "            break\n",
    "\n",
    "    return (phi, psi), iter_count\n",
    "\n",
    "# Test the method with 100 random initial theta values\n",
    "num_samples = 100\n",
    "y_values = np.random.uniform(5, 10, (num_samples, 3))\n",
    "a, b, c = np.random.uniform(1, 5, (3,))\n",
    "total_iters = 0\n",
    "\n",
    "for i in range(num_samples):\n",
    "    y = y_values[i]\n",
    "    phi_initial, psi_initial = np.random.uniform(0, np.pi/2, (2,))\n",
    "    polar_params = (phi_initial, psi_initial)\n",
    "    x_0, iter_count = newton_method_grad_descent(f_x_polar, gradient_f_x_polar, hessian_f_x_polar, polar_params, (a, b, c), y, 1000)\n",
    "    total_iters += iter_count\n",
    "\n",
    "avg_iters = total_iters / num_samples\n",
    "print(\"Average number of iterations needed:\", avg_iters)\n"
   ],
   "metadata": {
    "id": "CRGezZjzDqDS",
    "ExecuteTime": {
     "end_time": "2024-02-16T22:53:41.238524Z",
     "start_time": "2024-02-16T22:53:38.840928Z"
    }
   },
   "execution_count": 64,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of iterations needed: 1000.0\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "(0.5543993146371464, 0.0)"
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-16T22:53:43.787238Z",
     "start_time": "2024-02-16T22:53:43.783053Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "In this implementation, we utilize the Newton method to find critical points of the objective function \\( f \\) parameterized by \\( \\phi \\) and \\( \\psi \\). Unlike gradient descent, the Newton method incorporates both gradient and Hessian matrix, enabling a more direct approach to critical points.\n",
    "\n",
    "We employ polar coordinates \\( (\\phi, \\psi) \\) to represent the parameter space, aligning with the problem's geometry. This choice offers advantages in geometrical problems, providing a natural representation of ellipsoids and curved surfaces.\n",
    "\n",
    "Our tailored implementation includes the objective function \\( f \\), its gradient, and its Hessian matrix, specifically designed for polar coordinates. These functions allow efficient optimization and critical point detection. By leveraging the Newton method's specificity to the problem's geometry and parameterization, we aim to achieve effective optimization results.\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "j2kT9RnepFCG"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.4 Comparison and conclusions (0.25 pts)\n",
    "\n",
    "### Task 2.5 Comparison and conclusion\n",
    "\n",
    "- Compare the above implementations &mdash; gradient descent for Cartesian coordinates, for spherical coordiantes, and Newton method &mdash; w.r.t. time efficiency, better results\n",
    "\n",
    "- Summarize in a few sentences what you have learned and achieved by completing the tasks of this assignment. Comment on how this assignment can be improved in the future\n",
    "\n"
   ],
   "metadata": {
    "id": "jIrYref_xcHX"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "\n",
    "\n",
    "In comparing the implementations, gradient descent for Cartesian coordinates is efficient but struggles with complex functions. Gradient descent for spherical coordinates suits geometry-based problems, offering faster convergence. The Newton method, despite complexity, achieves faster convergence by using more information. The choice depends on the problem's nature, with spherical coordinates ideal for geometry and the Newton method for smoother functions.\n",
    "\n",
    "Through this assignment, I've explored optimization techniques and their applications. I've learned about parameterization choices' impact and the trade-offs between simplicity and efficiency.\n",
    "\n",
    "I also used theoretical homework as a background for tasks 1 and 2, for cartesian and polar coordinates gradient descends. It was much easier to implement the code, knowing which functions and gradients of them to utilize, thanks!\n",
    "\n",
    "---"
   ],
   "metadata": {
    "id": "vl1BrBuQpJYT"
   }
  }
 ]
}
